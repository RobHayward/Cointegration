\documentclass{article}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}
\title{Cointegration}
\author{Rob Hayward}
\begin{document}
\maketitle
\section{Introduction}
This paper will examine time series with a focus on stationarity, integration and cointegration. The first part discusses tests for stationarity, the second looks at cointegration and the methods used to identify cointegrated series. 

\section{Stationary data}
A standard series to be investigated can take the form of 
\begin{equation}
y_t = TD_t + z_t
\end{equation}

Where $y_t$ is the series of attention, $TD_t$ is the deterministic component that takes the form of $TD_t = \beta_0 + \beta_1 t$ and $z_t$ is the stochastic part that is assumed to be an autoregressive-moving average process of the form $\Phi L(z_t) = \theta L(z_t) \varepsilon_t$,  with $\varepsilon_t \sim iid$. 

It is possible to differentiate between \emph{trend stationary} 

\begin{equation}
y_t = y_{t-1} + \mu = y_0 + \mu t
\end{equation}

and \emph{difference stationary} processes.  

\begin{equation}
y_t = y_{t-1} + \varepsilon = y_0 + \sum_{i=0}^t \varepsilon_t
\end{equation}

If all the roots of the \emph{characteristic function}\footnote{This is of the form $\Pi(z) = \Pi_1(z) - \Pi_2(z)^2 - ... - \Pi_n(z)^n$} lie outside the unit circle or all the eigene roots of the \emph{companion function}\footnote{This takes the form $\mathbf{x_t} = \mathbf{\Pi x_{t-1}} + \mathbf{e_t}$} lie within the unit circle, the process is stationary; if at least one of the roots lies on the unit circle and there is a unit root and then the process is difference statonary. 

\begin{equation}
\phi_p(z) = 1 - \phi_1 (z) - \phi_2(z)^2 - \phi_3(z)^3...\phi_p(z)^p
\end{equation}
 
It is possible to create and plot these different types of time series.  
<<ur >>=
set.seed(123456)
e <- rnorm(500)
rw.nd <- cumsum(e)
@
After setting the seed and generating 500 norman random variables $(e)$. 
<<rwwd>>=
trd <- 1:500
@
The \emph{random walk} $(rw.nd)$ is the cumulation $(cumsum(e))$ of the normal random variable.
<<rw>>=
rw.wd <- 0.5*trd + cumsum(e)
@
By creating a trend $(trd)$ a \emph{random walk with drift} can be established with a combination of the cumulative shock and a constant drift $(rw.wd)$.
<<dt>>=
dt <- e + 0.5*trd
@
A \emph{deterministic trend with noise} $(dt)$ combines the tend $(trd)$ with noise $(e)$.

Now plot the three series. 
<<plotting, fig.height=6, fig.width=6, fig.cap="Three Series", fig.pos="h">>=
par(mar=rep(5,4))
plot.ts(dt, lty=1, ylab='', xlab='')
lines(rw.wd, lty=2)
par(new=T)
plot.ts(rw.nd, lty=3, axes=FALSE)
axis(4, pretty(range(rw.nd)))
lines(rw.nd, lty=3)
legend(10, 18.7, legend=c('det. trend + noise (ls)', 
                          'rw drift (ls)', 'rw (rs)'), lty=c(1, 2, 3))
@
There are also a series of tests that can be used to determine the nature of the time series.  There are three types of statonary series to be identified:  \emph{trend stationary}, \emph{difference stationary} and \emph{difference stationary with drift}.

\subsection{Dickey-Fuller Tests}

Equation \ref{eq:gen} can be used to estimate all three types of series. 
\begin{equation}
\label{eq:gen}
y_t = \beta_1 + \beta_2 t + \rho y_{t-1} +\sum_{j=1}^k \gamma_j \Delta y_i + u_{1t}
\end{equation}
However, rather than testing the unit root as $\rho$ being equal to unity, it is more usual to take $y_{t-1}$ is taken from each side to produce the following adaption of Equation \ref{eq:gen}. 

\begin{equation}
\Delta y_t = \beta_1 + \beta_2 t + \pi y_{t-1} +\sum_{j=1}^k \gamma_j \Delta y_i + u_{1t}
\label{eq:df}
\end{equation}

where $\pi = 1 - \rho$ and therefore if $\pi$ is significantly different from zero, $\rho$ cannot be one and there is no unit root. 

Lags of the dependent variable are used to remove any serial correlation in the residuals. \emph{Information Criteria} and t-statistics can be used to assess the appropriate number of lags

Using the usca package and the ur.df function on UK real consumer spending data (lc). Set up the data as a timeseries.   

<<DF>>=
library(urca)
library(xtable)
data(Raotbl3)
lc <- ts(Raotbl3$lc, start=c(1966,4), end=c(1991,2), frequency=4)
@
<<plotlc, echo=FALSE, fig.cap="Log UK Consumer Spending", fig.pos="h">>=
plot.ts(lc, main = "UK Real Consumer Spending", ylab = "")
@
Conduct the Augmented Dickey-Fuller test on (lc.ct) trend and (lc.co) drift using three lags. 
<<DF1>>=
lc.ct <- ur.df(lc, lags=3, type='trend')
lc.co <- ur.df(lc, lags=3, type='drift')
@
The three different equations are tested by 'trend', 'drift' or 'none' and there are two tests that take place. 

The first $(\tau_3)$ tests whether $\pi$ is equal to zero.  The test is the usual t-value on the lagged dependent variable. This can seen in the summary() function or the SlotName "teststat".  The critical values for the test statistics are in the slotName "cval".  

\begin{equation}
\Delta y_t = \beta_0 + \beta_1 t + \pi y_{t-1} + \sum_{j=1}^k \gamma_j \delta y_{t-j}
\end{equation}

The following code extracts the relevant values and puts them into a table. 
<<table, results='asis'>>=
a <- cbind(t(lc.ct@teststat), lc.ct@cval)
print(xtable(a, digits = 2, caption = "DW and F-tests"))
@

The $\tau_3$ test statistic is the test of the null hypothesis that the coefficient on the difference of the lagged dependent variable is equal to zero and that there is a \emph{unit root} as $\rho$ is equal to one.  


The critical value for a sample size of 100 comes from \citep{Fuller1976}. 

An F-test of the null hypothesis that the coefficients on the lagged change in the dependend variable and the coefficient on the time trend are jointly equal to zero is also supplied $(\phi_3)$.  The critical values come from Table VI \citep{DF1981} testing the null $(\alpha, \beta, \rho) = (\alpha, 0, 1)$.  It seems that unit root and lack of time trend cannot be rejected. A joint test of the null that the coefficients on the drift, time trend and lagged difference of the dependent variable is suppoed in $(\phi_2)$.  The critical values come from Table V \citep{DF1981} testing the null $(\alpha, \beta, \rho) = (0, 0, 1)$.


As the consumption series does not appear to be trend stationary, a test without the trend can be carried out.  This is equivalent to setting $\beta_2$ in Equation \ref{eq:gen} to zero. lc.co is the test of the series with drift.  

The F-Test is of the hypothesis $(\beta_1, \beta_2, \pi) = (\beta_1, 0, 0)$. It is conducted by imposing restrictions on the equation and assesses whether these restrictions have a significant effect on the residuals.  The nulll hypothesis is that the restrictions hold.  


<<table2, results='asis'>>=
a <- cbind(t(lc.co@teststat), lc.co@cval)
print(xtable(a, digits = 2, caption = "DW and F-tests 2"))
@
The critical value of 2.88 $(\phi_1)$ is a test of the null that the coefficients on the drift and lagged difference of the dependend variable are jointly equal to zero.  This cannot be rejected.  Therefore, it seems that the log of UK consumer spending is a random walk.  

To complete the picture, the change in consumer spending is tested to maker sure that the series are I(1) rather than I(2). First create the difference series lc2.ct.  
<<Diff>>=
lc2 <- diff(lc)
lc2.ct <- ur.df(lc2, type='trend', lags=3)
@

<<table3, results='asis'>>=
a <- cbind(t(lc2.ct@teststat), lc2.ct@cval)
print(xtable(a, digits = 2, caption = "DW and F-tests 3"))
@
This shows that the null of a unit root can be rejected and indicates that the UK consumer spending data are difference stationary.  

\subsection{KPSS}
There are a number of other tests of a unit root in the Bernhard Pfaff text (pages 94 to 102).  These include the \emph{Phillips-Peron}, \emph{Elliot-Rothenberg-Stock} and \emph{Schmidt-Phillips} tests which are implemented by ur.pp, ur.ers and ur.sp respectively in the urca package.  However, these all test the null of a unit root.  The \emph{Kwiatkowski-Phillips-Schmidt-Shin Test} \citep{KPSS} tests the null stationarity.  This is a much more powerful test and can be used in conjunction with the more conventional tests.  If the other tests suggest a unit root but the KPSS rejects a unit root, it is probably best to consider the data as stationary. 

The KPSS test is of the form
\begin{equation}
\label{eq:KPSS}
y_t = \zeta t + r_t + \varepsilon_t
\end{equation}

\begin{equation}
r_t = r_{t-1} + u_t
\end{equation}

The test statistic is calculated by running the regression of $y$ on a constant and trend as in Equation \ref{eq:KPSS} or on just a constant as in equation \ref{eq:KPSS} with $\zeta$ equal to zero.    

\begin{equation}
LM = \frac{\sum_{i = 1}^T S_t^2}{\hat{\sigma_t^2}}
\end{equation}
where 
\begin{equation}
S_t = \sum_{i = 1}^t \hat{\varepsilon} , t = 1, 2,...T
\end{equation}
and the estimate of the error variance
\begin{equation}
\hat{\sigma_{\varepsilon}}^2 = s^2 (l) = T^{-1} \sum_{t=1}^T \varepsilon_t^2 +2T - 1 \sum_{s=1}^l 1-\frac{s}{l+1} \sum_{t=s+1}^T \hat{\varepsilon} \hat{\varepsilon}_{t-1}
\end{equation}

Using the urca package and the data for US interest rates and nominal wages, the KPSS test is either on level statonary (type = $\mu$) or trend stationary (type = $\tau$) and the lags for the error term are either specified (as below) or set to "short" $\root 4 \of {4 \times (n/100)}$ or "long" $\root 4 \of {12 \times (n/100)}$.
<<KPSS-data>>=
data(nporg)
ir <- na.omit(nporg[, "bnd"])
wg <- log(na.omit(nporg[, "wg.n"]))
@
Plot the data
<<plot-kpss, fig.cap="US interest rate and wage data", fig.pos="h">>=
par(mfrow=c(2,1))
plot.ts(ir, main = "US interest rates")
plot.ts(wg, main = "US nominal wages")
@

<<kpss-test>>=
ir.kpss <- ur.kpss(ir, type = "mu", use.lag=8)
wg.kpss <- ur.kpss(wg, type = "tau", use.lag=8)
@
And the appropriate data can be extracted and placed into a table using the following. 
<<kpss-table, results='asis'>>=
a <- cbind(ir.kpss@teststat, ir.kpss@cval)
b <- cbind(wg.kpss@teststat, wg.kpss@cval)
ab <- rbind(a, b)
colnames(ab) <- c("CV", "10pct", "5pct", "2.5pct", "1.0pct")
rownames(ab) <- c("ir", "wg")
print(xtable(ab, digits = 2, caption = "KPSS and critical values"))
@
This shows that the null hypothesis of level stationarity for the interest rate series and trend stationarity for the wage series cannnot be rejected.  

\subsection{Dealing with lack of stationarity}
If the data are trend-stationary, one way to deal with the lack of stationarity would be to remove the trend.  One method is described in the footnote on page 53 of Pfaff. This takes the residuals from a regression of a series that is the same length as the log of consumption.   
<<Detrend>>=
detrended <- residuals(lm(lc ~ seq(along = lc)))
@
Which takes the following form. 
<<Plot-Detrend, fig.pos="h", fig.cap="Detrended Plot">>=
plot(detrended, type = 'l', main = "De-trended series")
@

\section{Cointegration}
This is the overview of conintegration and the methods use to analyse conintegrated relationships. Non-stationary data may exhibit \emph{spurious regression}.  If two norman random variables are created (e1 and e2) and two series (y1 and y2) have a trend plus a random shock. 
<< intro, error = FALSE, warnings = FALSE, message = FALSE >>=
library(lmtest)
library(xtable)
set.seed(123456)
e1 <- rnorm(500)
e2 <- rnorm(500)
trd <- 1:500
y1 <- 0.8*trd + cumsum(e1)
y2 <- 0.6*trd + cumsum(e2)
@
Now plot the two series. 

<<plot, fig.caption = "Plot of y1 and y2">>=
plot(y1, type = 'l', main = "Plot of y1 and y2", 
     lty = 1,  ylab = 'y1, y2')
lines(y2, lty = 2)
legend("topleft", legend = c('y1', 'y2'), lty = c(1,2))
@

Run a regression of $y1$ on $y2$ and it appears that there is a strong relationship.  
<<Regression, results='asis'>>=
sr.reg <- lm(y1 ~ y2)
print(xtable(sr.reg, caption = "Regresson results"))
@
However, the Durbin-Watson statistics shows there is a large amount of auto-correlation in the residuals.  
<<DW>>=
sr.dw <- dwtest(sr.reg)$statistic
sr.dw
@
The statistic will be around 2 if there is no autocorrelation. As a general rule, there are groups for suspicion if the $R^2$ is larger that the Durbin-Watson statistic. 

The main idea of cointegration is that a combnation of one or more non-stationary variables will show a stationary relationship.  Pfaff provides the following definition. 
\begin{quotation}
`` The components of a vector $\mathbf{x_t}$, are said to be cointegrated of order b, d; denoted $x \sim CI(b,d)$ if (a) all components of $x_t$ are I(d) and (b) a vector $\alpha (\neq 0)$ exists so that $z_t = \alpha'x_t \sim I(d - b), b > 0$.  The vector $\alpha$ is called the conintegrating vector.''
\end{quotation}
\citep[p. 75]{varsb}

If two or more non-stationary series are cointegrated, a linear combination of the two may be cointegrated and this combination can be included in the regression.  The aim is to have a system of the form
\begin{subequations}
\begin{align}
\Delta y_t &= \psi_0 + \gamma_1 z_{t-1} + \sum_{i=1}^k \psi_i \Delta x_{t-i} +\sum_{i=1}^k \psi_i \Delta y_{t-i} + \varepsilon_{1, t}\\
\Delta x_t &= \psi_0 + \gamma_1 z_{t-1} + \sum_{i=1}^k \psi_i \Delta x_{t-i} +\sum_{i=1}^k \psi_i \Delta y_{t-i} + \varepsilon_{2, t}
\end{align}
\end{subequations}
Where $z$ is the cointegrated relationship.  $y$ and $x$ are difference stationary.  One way to estimate this model is to use the two-step \emph{Engle-Granger} method \citep{EG1987}. 

For an example of this, create two non-stationary series $(y1)$ and $(y2)$ with a long-run relationship where $y2$ is equal to 0.6 $y1$.   
<<EG, echo=TRUE>>=
set.seed(123456)
e1 <- rnorm(100)
e2 <- rnorm(100)
y1 <- cumsum(e1)
y2 <- 0.6*y1 + e2
@
Plot these series.
<<Plot-EG, fig.cap="Plot y1 and y2">>=
plot(y1, type ='l', lty = 1, main = "Plot y1 and y2")
lines(y2, lty = 2)
legend("topleft", legend = c('y1', 'y2'), lty = c(1,2))
@
Now run the regression on the long-run relationship and save the residuals from that regression.  The residuals are the deviations from the long run relationship.  
<<EG2>>=
lr.reg <- lm(y2 ~ y1)
error <- residuals(lr.reg)
@
The residual show the divergence from the long run relationship between y1 and y2. 
<<Plot-EG2, fig.cap="Plot Error">>=
plot(error, type = 'l', main = "Divergence from long-run y1-y2 relationship")
@
Now create the lagged error term and differences in y1 and y2 to allow each variable to respond to the deviation from the long-run relationship.  The embed() function will created the lagged dataframe.  
<<EG3>>=
error.lagged <- error[-c(1, 100)]
dy1 <- diff(y1)
dy2 <- diff(y2)
diff.dat <- data.frame(embed(cbind(dy1, dy2), 2))
colnames(diff.dat) <- c('dy1', 'dy2', 'dy1.1', 'dy2.1')
@
<<EG-Reg, results='asis'>>=
ecm.reg <- lm(dy2 ~ error.lagged + dy1.1 + dy2.1, data=diff.dat)
print(xtable(summary(ecm.reg), caption = 'Engle-Granger Regression Result'))
@
The results show that most of the disturbance from equilibrium is corrected swiftly with the coefficient on the lagged error at 0.97. 

\subsection{Use of cointegration}
These cointegration techniques can be used to test whether there is a stable relationship between variables.  For example, one test could be whether cash and futures prices are cointegrated.  A Dickey-Fuller test cannot be made of the residuals of a regression of cash on futures because the process of creating the residuals through the regression will bias the result by selecting parameters that reduce the variability of the estimated errors as much as possible.  Therefore, the Engle-Granger residuals should be used.  These are interpolations from the response surface in \citep{MacKinnon1996}.  These are in Table C \citep[p. 490]{EndersTS}.  

If the tests indicate that the residuals from the regression are stationary, the coefficient from this regression gives the relationship between cash a futures. Similarly, the efficiecy of pairs trades can be assessed with the error-correction model. This also gives an indication of how swiftly the relationship will return to normal after a disturbance. 

This technique could also be used to make a pairs test of two securities.  The cointegration is the relationship between the two and the error correction can show the speed at which the relationship will return to the long run equilibrium.  Alternatively, an analysis of the bond market could use the yield curve (long term and short term interest rates) as a cointegrating factor.  

\subsection{Example}
This examples uses the artificial data from the Enders textbook \citep[pp. 377-379]{EndersTS}.  The simulated series are created in the following way.

\begin{tabular}[l]{l | c | c | c}
 &  $(y_t)$ & $(z_t)$ & $(w_t)$ \\
 \hline
 Trend & $\mu_{yt} = \mu_{yt} + \varepsilon_{zt}$ & $\mu_{zt} = \mu_{zt} + \varepsilon_{zt}$ & $\mu_{wt} = \mu_{yt} + \mu_{zt}$\\
 Irregular & $\delta_{yt} = 0.5 \delta_{yt-1} + \eta_{yt}$ & $\delta_{zt} =0.5 \delta_{zt-1} + \eta_{zt}$ & $\delta_{wt} = 0.5 \delta_{wt-1} + \eta_{wt}$\\
 Series & $y_t = \mu_{yt} +\delta_{yt}$ & $z_t = \mu_{zt} + \delta_{zt}$ & $w_t = \mu_{wt} + \delta_{wt} + 0.5 \delta_{yt} + 0.5 \delta_{zt}$
 \end{tabular}
 
The data are available on the \href{http://bcs.wiley.com/he-bcs/Books?action=resource&bcsId=5276&itemId=0470505397&resourceId=18503}{Enders companion web site}.  Load the data and plot.
<<loaddata>>=
da <- read.csv("coint6.csv", header = TRUE)
head(da)
@
Plot data
<<Plot4>>=
plot(da$z, lty=1, ylab='', xlab='', type = 'l', 
     main = "Plot of y, x and w", ylim = c(-10, 2))
lines(da$w, lty=2)
lines(da$y, lty = 3)
legend("topright", legend=c('y', 'z', 'w'), 
                          lty=c(1, 2, 3))
@
Now test whether the series are stationary using the augmented Dickey-Fuller test.  
<<Reg, results = 'asis'>>=
eq1 <- lm(da$y ~ da$z + da$w)
print(xtable(summary(eq1), caption = 'Least Squares y on z and w'))
@
This can be repeated for each of the three possible relationships and the errors that are collected can be saved and tested for stationarity.
<<res>>=
resy <- eq1$residuals
eq2 <- lm(da$z ~ da$y + da$w)
resz <- eq2$residuals
eq3 <- lm(da$w ~ da$z + da$y)
resw <- eq3$residuals
@
The test is of the form.
\begin{equation}
\Delta y_t = \alpha_0 + \alpha_1 y_{t-1} + \sum_{i=1}^p \alpha_{i+1} \Delta y_{t-1} + \varepsilon_{t}
\end{equation}

<<EGrestest, echo=FALSE, results='asis', cache=TRUE>>=
eq1 <- lm(y ~ z + w, data = da)
resy <- eq1$residuals
eq2 <- lm(z ~ y + w, data = da)
resz <- eq2$residuals
eq3 <- lm(w ~ z + y, data = da)
resw <- eq3$residuals
y0 <- ur.df(resy, type = "none", lags = 0)
z0 <- ur.df(resz, type = "none", lags = 0)
w0 <- ur.df(resw, type = "none", lags = 0)
y4 <- ur.df(resy, type = "none", lags = 4)
z4 <- ur.df(resz, type = "none", lags = 4)
w4 <- ur.df(resw, type = "none", lags = 4)
M <-matrix(0, nrow = 6, ncol = 2)
M[1:2,1] <- t(coefficients(y0@testreg)[1,c(1,3)])
M[3:4,1] <- t(coefficients(z0@testreg)[1,c(1,3)])
M[5:6,1] <- t(coefficients(w0@testreg)[1,c(1,3)])
M[1:2,2] <- t(coefficients(y4@testreg)[1,c(1,3)])
M[3:4,2] <- t(coefficients(z4@testreg)[1,c(1,3)])
M[5:6,2] <- t(coefficients(w4@testreg)[1,c(1,3)])
colnames(M) <-c("No Lags", "4 Lags")
rownames(M) <-c("e_yt", "SE(y)", "e_zt", "SE(z)", "e_wt", "SE(w)" )
print(xtable(M, caption = 'Estimated $a_1$', digits = 4))
@
The figures displayed here are the same as those in Table 6.2 of Enders \citep[p. 379]{EndersTS}.  The critical value comes from MacKinnon.  The programme does not appear to give as much flexibility as the talbes in Enders.  Sample code below. 
<<MacKinnon, results='asis'>>=
xtable(unitrootTable(trend = "nc"), digits = 4, caption = 'MacKinnon unit root table')
@

\subsection{Johansen Procedure}
When there are more than two series there may still be a relationship that creates a stationary, linear combination that can be used in a regresssion.  

For example, a simulated series can be created in the following fashion. First create the three series and put them into a dataframe.  
<<jj>>=
set.seed(12345)
e1 <- rnorm(250, 0, 0.5)
e2 <- rnorm(250, 0, 0.5)
e3 <- rnorm(250, 0, 0.5)
u1.ar1 <- arima.sim(model = list(ar = 0.75),
                    innov = e1, n = 250)
u2.ar1 <- arima.sim(model = list(ar = 0.3),
                    innov = e2, n = 250)
y3 <- cumsum(e3)
y1 <- 0.8 * y3 + u1.ar1
y2 <- -0.3 * y3 + u2.ar1
y.mat <- data.frame(y1, y2, y3)
@
Take a look at the series that have been created. 
<<plot3, fig.cap="Three Simulated Series", fig.pos="h">>=
plot(y3, main = "Three series", lty = 3, type = 'l')
lines(y2, lty = 1, type = 'l')
lines(y1, lty = 2, type = 'l')
legend("topleft", legend=c('y1', 'y2', 'y3'), 
       lty = c(1, 2, 3))
@
<<vecm>>=
require(xtable)
vecm <- ca.jo(y.mat)
jo.results <- summary(vecm)
@
<<table4, results='asis'>>=
a <- cbind(jo.results@teststat,jo.results@cval)
colnames(a) <- c("CV", "10pct", "5pct", "1pct")
print(xtable(a, digits = 2, caption = 'Johansen Test and Critical Values'))
@
\newpage
\bibliography{myref}
\bibliographystyle{agsm}


\end{document}