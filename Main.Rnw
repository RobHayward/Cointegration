\documentclass{article}
\usepackage{amsmath}
\usepackage{natbib}
\title{Cointegration}
\author{Rob Hayward}
\begin{document}
\maketitle
\section{Introduction}
This paper will examine time series with a focus on stationarity, integration and cointegration. The first part discusses tests for stationarity, the second looks at cointegration and the methods used to identify cointegrated series. 

\section{Stationary data}
A standard series to be investigated can take the form of 
\begin{equation}
y_t = TD_t + z_t
\end{equation}

Where $y_t$ is the series of attention, $TD_t$ is the deterministic component that takes the form of $TD_t = \beta_0 + \beta_1 t$ and $z_t$ is the stochastic part that is assumed to be an autoregressive-moving average process of the form $\Phi L(z_t) = \theta L(z_t) \varepsilon_t$,  with $\varepsilon_t \sim iid$. 

It is possible to differentiate between \emph{trend stationary} 

\begin{equation}
y_t = y_{t-1} + \mu = y_0 + \mu t
\end{equation}

and \emph{difference stationary} processes.  

\begin{equation}
y_t = y_{t-1} + \varepsilon = y_0 + \sum_{i=0}^t \varepsilon_t
\end{equation}

If all the roots of the autoregressive polynominal $\phi_p(z)$ lie outside the unit circle, the process is stationary (possibly trend stationary); if at least one of the roots lies on the unit circle and there is a unit root and then the process is difference statonary. 

\begin{equation}
\phi_p(z) = 1 - \phi_1 (z) - \phi_2(z)^2 - \phi_3(z)^3...\phi_p(z)^p
\end{equation}
 
It is possible to create and plot these different types of time series.  
<<ur >>=
set.seed(123456)
e <- rnorm(500)
rw.nd <- cumsum(e)
@
After setting the seed and generating 500 norman random variables $(e)$. 
<<rwwd>>=
trd <- 1:500
@
The \emph{random walk} $(rw.nd)$ is the cumulation $(cumsum(e))$ of the normal random variable.
<<rw>>=
rw.wd <- 0.5*trd + cumsum(e)
@
By creating a trend $(trd)$ a \emph{random walk with drift} can be established with a combination of the cumulative shock and a constant drift $(rw.wd)$.
<<dt>>=
dt <- e + 0.5*trd
@
A \emph{deterministic trend with noise} $(dt)$ combines the tend $(trd)$ with noise $(e)$.

Now plot the three series. 
<<plotting, fig.height=6, fig.width=6, fig.cap="Three Series", fig.pos="h">>=
par(mar=rep(5,4))
plot.ts(dt, lty=1, ylab='', xlab='')
lines(rw.wd, lty=2)
par(new=T)
plot.ts(rw.nd, lty=3, axes=FALSE)
axis(4, pretty(range(rw.nd)))
lines(rw.nd, lty=3)
legend(10, 18.7, legend=c('det. trend + noise (ls)', 
                          'rw drift (ls)', 'rw (rs)'), lty=c(1, 2, 3))
@
There are also a series of tests that can be used to determine the nature of the time series.  There are three types of statonary series to be identified:  \emph{trend stationary}, \emph{difference stationary} and \emph{difference stationary with drift}.

\subsection{Dickey-Fuller Tests}

Equation \ref{eq:gen} can be used to estimate all three types of series. 
\begin{equation}
\label{eq:gen}
y_t = \beta_1 + \beta_2 t + \rho y_{t-1} +\sum_{j=1}^k \gamma_j \Delta y_i + u_{1t}
\end{equation}
However, rather than testing the unit root as $\rho$ being equal to unity, it is more usual to take $y_{t-1}$ is taken from each side to produce the following adaption of Equation \ref{eq:gen}. 

\begin{equation}
\Delta y_t = \beta_1 + \beta_2 t + \pi y_{t-1} +\sum_{j=1}^k \gamma_j \Delta y_i + u_{1t}
\label{eq:df}
\end{equation}

where $\pi = 1 - \rho$ and therefore if $\pi$ is significantly different from zero, $\rho$ cannot be one and there is no unit root. 

Lags of the dependent variable are used to remove any serial correlation in the residuals. \emph{Information Criteria} and t-statistics can be used to assess the appropriate number of lags

Using the usca package and the ur.df function on UK real consumer spending data (lc). Set up the data as a timeseries.   

<<DF>>=
library(urca)
library(xtable)
data(Raotbl3)
lc <- ts(Raotbl3$lc, start=c(1966,4), end=c(1991,2), frequency=4)
@
<<plotlc, echo=FALSE, fig.cap="Log UK Consumer Spending", fig.pos="h">>=
plot.ts(lc, main = "UK Real Consumer Spending", ylab = "")
@
Conduct the Augmented Dickey-Fuller test on (lc.ct) trend and (lc.co) drift using three lags. 
<<DF1>>=
lc.ct <- ur.df(lc, lags=3, type='trend')
lc.co <- ur.df(lc, lags=3, type='drift')
@
The three different equations are tested by 'trend', 'drift' or 'none' and there are two tests that take place. 

The first $(\tau_3)$ tests whether $\pi$ is equal to zero.  The test is the usual t-value on the lagged dependent variable. This can seen in the summary() function or the SlotName "teststat".  The critical values for the test statistics are in the slotName "cval".  The following code extracts the relevant values and puts them into a table. 
<<table, results='asis'>>=
a <- cbind(t(lc.ct@teststat), lc.ct@cval)
print(xtable(a, digits = 2, caption = "DW and F-tests"))
@

The $\tau_3$ test statistic is the test of the null hypothesis that the coefficient on the difference of the lagged dependent variable is equal to zero and that there is a \emph{unit root} as $\rho$ is equal to one.  


The critical value for a sample size of 100 comes from \citep{Fuller1976}. 

An F-test of the null hypothesis that the coefficients on the lagged change in the dependend variable and the coefficient on the time trend are jointly equal to zero is also supplied $(\phi_3)$.  The critical values come from Table VI \citep{DF1981} testing the null $(\alpha, \beta, \rho) = (\alpha, 0, 1)$.  It seems that unit root and lack of time trend cannot be rejected. A joint test of the null that the coefficients on the drift, time trend and lagged difference of the dependent variable is suppoed in $(\phi_2)$.  The critical values come from Table V \citep{DF1981} testing the null $(\alpha, \beta, \rho) = (0, 0, 1)$.

As the consumption series does not appear to be trend stationary, a test without the trend can be carried out.  This is equivalent to setting $\beta_2$ in Equation \ref{eq:gen} to zero. lc.co is the test of the series with drift.  
<<table2, results='asis'>>=
a <- cbind(t(lc.co@teststat), lc.co@cval)
print(xtable(a, digits = 2, caption = "DW and F-tests 2"))
@
The critical value of 2.88 $(\phi_1)$ is a test of the null that the coefficients on the drift and lagged difference of the dependend variable are jointly equal to zero.  This cannot be rejected.  Therefore, it seems that the log of UK consumer spending is a random walk.  

To complete the picture, the change in consumer spending is tested to maker sure that the series are I(1) rather than I(2). First create the difference series lc2.ct.  
<<Diff>>=
lc2 <- diff(lc)
lc2.ct <- ur.df(lc2, type='trend', lags=3)
@

<<table3, results='asis'>>=
a <- cbind(t(lc2.ct@teststat), lc2.ct@cval)
print(xtable(a, digits = 2, caption = "DW and F-tests 3"))
@
This shows that the null of a unit root can be rejected and indicates that the UK consumer spending data are difference stationary.  

\subsection{KPSS}
There are a number of other tests of a unit root in the Bernhard Pfaff text (pages 94 to 102).  These include the \emph{Phillips-Peron}, \emph{Elliot-Rothenberg-Stock} and \emph{Schmidt-Phillips} tests which are implemented by ur.pp, ur.ers and ur.sp respectively in the urca package.  However, these all test the null of a unit root.  The \emph{Kwiatkowski-Phillips-Schmidt-Shin Test} \citep{KPSS} tests the null stationarity.  This is a much more powerful test and can be used in conjunction with the more conventional tests.  If the other tests suggest a unit root but the KPSS rejects a unit root, it is probably best to consider the data as stationary. 

The KPSS test is of the form
\begin{equation}
\label{eq:KPSS}
y_t = \zeta t + r_t + \varepsilon_t
\end{equation}

\begin{equation}
r_t = r_{t-1} + u_t
\end{equation}

The test statistic is calculated by running the regression of $y$ on a constant and trend as in Equation \ref{eq:KPSS} or on just a constant as in equation \ref{eq:KPSS} with $\zeta$ equal to zero.    

\begin{equation}
LM = \frac{\sum_{i = 1}^T S_t^2}{\hat{\sigma_t^2}}
\end{equation}
where 
\begin{equation}
S_t = \sum_{i = 1}^t \hat{\varepsilon} , t = 1, 2,...T
\end{equation}
and the estimate of the error variance
\begin{equation}
\hat{\sigma_{\varepsilon}}^2 = s^2 (l) = T^{-1} \sum_{t=1}^T \varepsilon_t^2 +2T - 1 \sum_{s=1}^l 1-\frac{s}{l+1} \sum_{t=s+1}^T \hat{\varepsilon} \hat{\varepsilon}_{t-1}
\end{equation}

Using the urca package and the data for US interest rates and nominal wages, the KPSS test is either on level statonary (type = $\mu$) or trend stationary (type = $\tau$) and the lags for the error term are either specified (as below) or set to "short" $\root 4 \of {4 \times (n/100)}$ or "long" $\root 4 \of {12 \times (n/100)}$.
<<KPSS-data>>=
data(nporg)
ir <- na.omit(nporg[, "bnd"])
wg <- log(na.omit(nporg[, "wg.n"]))
@
Plot the data
<<plot-kpss, fig.cap="US interest rate and wage data", fig.pos="h">>=
par(mfrow=c(2,1))
plot.ts(ir, main = "US interest rates")
plot.ts(wg, main = "US nominal wages")
@

<<kpss-test>>=
ir.kpss <- ur.kpss(ir, type = "mu", use.lag=8)
wg.kpss <- ur.kpss(wg, type = "tau", use.lag=8)
@
And the appropriate data can be extracted and placed into a table using the following. 
<<kpss-table, results='asis'>>=
a <- cbind(ir.kpss@teststat, ir.kpss@cval)
b <- cbind(wg.kpss@teststat, wg.kpss@cval)
ab <- rbind(a, b)
colnames(ab) <- c("CV", "10pct", "5pct", "2.5pct", "1.0pct")
rownames(ab) <- c("ir", "wg")
print(xtable(ab, digits = 2, caption = "KPSS and critical values"))
@
This shows that the null hypothesis of level stationarity for the interest rate series and trend stationarity for the wage series cannnot be rejected.  

\subsection{Dealing with lack of stationarity}
If the data are trend-stationary, one way to deal with the lack of stationarity would be to remove the trend.  One method is described in the footnote on page 53 of Pfaff. This takes the residuals from a regression of a series that is the same length as the log of consumption.   
<<Detrend>>=
detrended <- residuals(lm(lc ~ seq(along = lc)))
@
Which takes the following form. 
<<Plot-Detrend, fig.pos="h", fig.cap="Detrended Plot">>=
plot(detrended, type = 'l', main = "De-trended series")
@

\section{Cointegration}
This is the overview of conintegration and the methods use to analyse conintegrated relationships. Non-stationary data may exhibit \emph{spurious regression}.  If two norman random variables are created (e1 and e2) and two series (y1 and y2) have a trend plus a random shock. 
<< intro, error = FALSE, warnings = FALSE, message = FALSE >>=
library(lmtest)
library(xtable)
set.seed(123456)
e1 <- rnorm(500)
e2 <- rnorm(500)
trd <- 1:500
y1 <- 0.8*trd + cumsum(e1)
y2 <- 0.6*trd + cumsum(e2)
@
Now plot the two series
<<plot, fig.caption = "Plot of y1 and y2">>=
plot(y1, type = 'l', main = "Plot of y1 and y2", 
     col = 'red',  ylab = 'y1, y2')
lines(y2, col = 'blue')
@
Run a regression of $y1$ on $y2$ and it appears that there is a strong relationship.  
<<Regression, results='asis'>>=
sr.reg <- lm(y1 ~ y2)
print(xtable(sr.reg, caption = "Regresson results"))
@
However, the Durbin-Watson statistics shows there is a large amount of auto-correlation in the residuals.  
<<DW>>=
sr.dw <- dwtest(sr.reg)$statistic
sr.dw
@
The statistic will be around 2 if there is no autocorrelation. As a general rule, there are groups for suspicion if the $R^2$ is larger that the Durbin-Watson statistic. 
The main idea of cointegration is that a combnation of one or more non-stationary variables will show a stationary relationship.  Pfaff provides the following definition. 
\begin{quotation}
`` The components of a vector $\mathbf{x_t}$, are said to be cointegrated of order b, d; denoted $x \sim CI(b,d)$ if (a) all components of $x_t$ are I(d) and (b) a vector $\alpha (\neq 0)$ exists so that $z_t = \alpha'x_t \sim I(d - b), b > 0$.  The vector $\alpha$ is called the conintegrating vector.''
\end{quotation}
\citep[p. 75]{varsb}

If two or more non-stationary series are cointegrated, a linear combination of the two may be cointegrated and this combination can be included in the regression.  The aim is to have a system of the form
\begin{subequations}
\begin{align}
\Delta y_t &= \psi_0 + \gamma_1 z_{t-1} + \sum_{i=1}^k \psi_i \Delta x_{t-i} +\sum_{i=1}^k \psi_i \Delta y_{t-i} + \varepsilon_{1, t}\\
\Delta x_t &= \psi_0 + \gamma_1 z_{t-1} + \sum_{i=1}^k \psi_i \Delta x_{t-i} +\sum_{i=1}^k \psi_i \Delta y_{t-i} + \varepsilon_{2, t}
\end{align}
\end{subequations}
Where $z$ is the cointegrated relationship.  $y$ and $x$ are difference stationary.  One way to estimate this model is to use the two-step \emph{Engle-Granger} method \citep{EG1987}. 

For an example of this, create two non-stationary series $(y1)$ and $(y2)$ with a long-run relationship where $y2$ is equal to 0.6 $y1$.   
<<EG, echo=TRUE>>=
set.seed(123456)
e1 <- rnorm(100)
e2 <- rnorm(100)
y1 <- cumsum(e1)
y2 <- 0.6*y1 + e2
@
Plot these series.
<<Plot-EG, fig.cap="Plot y1 and y2">>=
plot(y1, type ='l', col = 'red', main = "Plot y1 and y2")
lines(y2, col = 'blue')
@
Now run the regression on the long-run relationship and save the residuals from that regression.  The residuals are the deviations from the long run relationship.  
<<EG2>>=
lr.reg <- lm(y2 ~ y1)
error <- residuals(lr.reg)
@
The residual show the divergence from the long run relationship between y1 and y2. 
<<Plot-EG2, fig.cap="Plot Error">>=
plot(error, type = 'l', main = "Divergence from long-run y1-y2 relationship")
@
Now create the lagged error term and differences in y1 and y2 to allow each variable to respond to the deviation from the long-run relationship.  The embed() function will created the lagged dataframe.  
<<EG3>>=
error.lagged <- error[-c(1, 100)]
dy1 <- diff(y1)
dy2 <- diff(y2)
diff.dat <- data.frame(embed(cbind(dy1, dy2), 2))
colnames(diff.dat) <- c('dy1', 'dy2', 'dy1.1', 'dy2.1')
@
<<EG-Reg, results='asis'>>=
ecm.reg <- lm(dy2 ~ error.lagged + dy1.1 + dy2.1, data=diff.dat)
print(xtable(summary(ecm.reg), caption = 'Engle-Granger Regression Result'))
@
The results show that most of the disturbance from equilibrium is corrected swiftly with the coefficient on the lagged error at 0.97. 



\newpage
\bibliography{myref}
\bibliographystyle{agsm}


\end{document}